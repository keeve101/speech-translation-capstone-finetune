{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio, concatenate_datasets\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"magichub_datasets\")\n",
    "\n",
    "SPKINFO = \"SPKINFO.txt\"\n",
    "UTTRANSINFO = \"UTTRANSINFO.txt\"\n",
    "AUDIOINFO = \"AUDIOINFO.txt\"\n",
    "\n",
    "dataset_dirs = os.listdir(root_dir)\n",
    "\n",
    "dataframes = {}\n",
    "speaker_infos = {}\n",
    "audio_infos = {}\n",
    "\n",
    "# deal with short-form dataset \n",
    "for dataset_dir in dataset_dirs:\n",
    "    if \"Scripted\" in dataset_dir:\n",
    "        clips_dir = root_dir / dataset_dir / \"clips\"\n",
    "        os.makedirs(clips_dir, exist_ok=True)\n",
    "\n",
    "        for root, dirs, files in os.walk(os.path.join(root_dir, dataset_dir)):\n",
    "            for f in files:\n",
    "                file_path = Path(root) / f\n",
    "                if f.endswith(\".wav\"):\n",
    "                    os.rename(file_path, clips_dir / f)\n",
    "                    \n",
    "                if f == UTTRANSINFO:\n",
    "                    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "                    dataframes[dataset_dir] = df\n",
    "\n",
    "                if f == SPKINFO:\n",
    "                    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "                    speaker_infos[dataset_dir] = df\n",
    "    elif \"Conversation\" in dataset_dir:\n",
    "        clips_dir = root_dir / dataset_dir / \"clips\"\n",
    "        os.makedirs(clips_dir, exist_ok=True)\n",
    "        \n",
    "        for root, dirs, files in os.walk(os.path.join(root_dir, dataset_dir)):\n",
    "            for f in files:\n",
    "                file_path = Path(root) / f\n",
    "                if f.endswith(\".txt\") and \"TXT\" in root:\n",
    "                    utterances = []\n",
    "                    base_name = f.split(\".\")[0]\n",
    "                    \n",
    "                    audio_path = Path(root).parent / \"WAV\" / f\"{base_name}.wav\"\n",
    "                    audio = AudioSegment.from_wav(audio_path)\n",
    "                    \n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        for i, line in enumerate(f):\n",
    "                            parts = line.strip().split(\"\\t\")\n",
    "                            assert len(parts) == 4\n",
    "                            start, end = map(float, parts[0][1:-1].split(\",\")) # e.g. [1.000, 2.000] --> 1.000, 2.000\n",
    "                            \n",
    "                            speaker_id = parts[1].strip()\n",
    "                            transcription = parts[3].strip()\n",
    "                            \n",
    "                            if speaker_id == \"0\": # \"0\" is system prompt\n",
    "                                continue\n",
    "                            \n",
    "                            segment = audio[start * 1000 : end * 1000]\n",
    "\n",
    "                            utterance_number = f\"U{i:04d}\"\n",
    "                            filename_prefix = f\"{base_name}_{utterance_number}\"\n",
    "                            \n",
    "                            segment.export(f\"{clips_dir}/{filename_prefix}.wav\", format=\"wav\")\n",
    "                            \n",
    "                            utterances.append({\n",
    "                                \"SPEAKER_ID\": speaker_id,\n",
    "                                \"TRANSCRIPTION\": transcription,\n",
    "                                \"UTTRANS_ID\": f\"{filename_prefix}.wav\",\n",
    "                            })\n",
    "                            \n",
    "                    if dataframes.get(dataset_dir) is None:\n",
    "                        dataframes[dataset_dir] = utterances\n",
    "                    else:\n",
    "                        dataframes[dataset_dir].extend(utterances)\n",
    "\n",
    "                elif f == SPKINFO:\n",
    "                    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "                    speaker_infos[dataset_dir] = df\n",
    "                \n",
    "                elif f == AUDIOINFO:\n",
    "                    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "                    audio_infos[dataset_dir] = df\n",
    "\n",
    "        dataframes[dataset_dir] = pd.DataFrame(dataframes[dataset_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CHANNEL', 'UTTRANS_ID', 'SPEAKER_ID', 'PROMPT', 'TRANSCRIPTION'], dtype='object')\n",
      "Index(['CHANNEL', 'SPEAKER_ID', 'GENDER', 'AGE', 'REGION,CITY', 'DEVICE'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dataframes[dataset_dirs[0]].columns)\n",
    "print(speaker_infos[dataset_dirs[0]].columns)\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for dataset_dir in dataset_dirs:\n",
    "    dataset = dataframes[dataset_dir]\n",
    "    speaker_info = speaker_infos[dataset_dir]\n",
    "    common_columns = list(set(dataset.columns) & set(speaker_info.columns))\n",
    "    dataset = pd.merge(dataset, speaker_info, on=common_columns)\n",
    "    \n",
    "    audio_info = audio_infos.get(dataset_dir)\n",
    "    if audio_info is not None:\n",
    "        audio_info.drop(columns=[\"UTTRANS_ID\"], inplace=True)\n",
    "        common_columns = list(set(audio_info.columns) & set(dataset.columns))\n",
    "        # dataset = pd.merge(dataset, audio_info, on=common_columns) # topic has duplicate rows, so merged rows get multiplied, just omit topic column entirely\n",
    "    \n",
    "    if \"REGION,CITY\" in dataset.columns or \"REGIONCITY\" in dataset.columns:\n",
    "        dataset = dataset.rename(columns={\"REGION,CITY\": \"REGION_CITY\"})\n",
    "        dataset = dataset.rename(columns={\"REGIONCITY\": \"REGION_CITY\"})\n",
    "    \n",
    "    datasets[dataset_dir] = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['CHANNEL', 'UTTRANS_ID', 'SPEAKER_ID', 'PROMPT', 'TRANSCRIPTION', 'GENDER', 'AGE', 'REGION_CITY', 'DEVICE'],\n",
      "    num_rows: 4073\n",
      "})\n",
      "Dataset({\n",
      "    features: ['SPEAKER_ID', 'TRANSCRIPTION', 'UTTRANS_ID', 'CHANNEL', 'GENDER', 'AGE', 'REGION_CITY', 'DEVICE'],\n",
      "    num_rows: 3149\n",
      "})\n",
      "Dataset({\n",
      "    features: ['CHANNEL', 'UTTRANS_ID', 'SPEAKER_ID', 'PROMPT', 'TRANSCRIPTION', 'GENDER', 'AGE', 'REGION_CITY', 'DEVICE'],\n",
      "    num_rows: 2242\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(datasets[dataset_dirs[0]])\n",
    "print(datasets[dataset_dirs[1]])\n",
    "print(datasets[dataset_dirs[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(datasets[dataset_dirs[0]]) == len(dataframes[dataset_dirs[0]])\n",
    "assert len(datasets[dataset_dirs[1]]) == len(dataframes[dataset_dirs[1]])\n",
    "assert len(datasets[dataset_dirs[2]]) == len(dataframes[dataset_dirs[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize columns\n",
    "\n",
    "for key, dataset in datasets.items():\n",
    "    for column in dataset.column_names:\n",
    "        datasets[key] = datasets[key].rename_column(column, column.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['channel', 'uttrans_id', 'speaker_id', 'prompt', 'transcription', 'gender', 'age', 'region_city', 'device'],\n",
      "    num_rows: 4073\n",
      "})\n",
      "{'channel': 'C1', 'uttrans_id': 'G0004_1_S0001.wav', 'speaker_id': 'G0004', 'prompt': 'So nag iwan sila ng ilang CDs tapos sabi ipatch nlng daw.', 'transcription': 'so nag-iwan sila ng ilang C Ds tapos sabi i-patch na lang daw', 'gender': 'M', 'age': 24, 'region_city': 'Cordillera Administrative Region, Baguio', 'device': 'AKG'}\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"Filipino_Scripted_Speech_Corpus_Daily_Use_Sentence\"])\n",
    "print(datasets[\"Filipino_Scripted_Speech_Corpus_Daily_Use_Sentence\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bff1edcaf9544648444b09b79f7cdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e7e8b938ed4503a192dc073f8c7d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b2dbf70db94d6f9361624fc0287847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mappings = {\n",
    "    \"transcription\": \"sentence\",\n",
    "    \"uttrans_id\": \"audio\"\n",
    "}\n",
    "for key, dataset in datasets.items():\n",
    "    for column_name, new_column_name in mappings.items():\n",
    "        if column_name in dataset.column_names:\n",
    "            datasets[key] = datasets[key].rename_column(column_name, new_column_name)\n",
    "    \n",
    "    if \"audio\" in datasets[key].column_names:\n",
    "        datasets[key] = datasets[key].map(lambda x: {\"audio\": f\"{key}/clips/{x['audio']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\keith\\Desktop\\repos\\finetune-whisper\\magichub_datasets\")\n",
    "for key, dataset in datasets.items():\n",
    "    datasets[key] = datasets[key].cast_column(\"audio\", Audio()) # cast all to 16kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['channel', 'audio', 'speaker_id', 'prompt', 'sentence', 'gender', 'age', 'region_city', 'device'],\n",
      "    num_rows: 4073\n",
      "})\n",
      "{'channel': 'C1', 'audio': {'path': 'Filipino_Scripted_Speech_Corpus_Daily_Use_Sentence/clips/G0004_1_S0001.wav', 'array': array([-0.00387573, -0.00564575, -0.01025391, ...,  0.        ,\n",
      "        0.        ,  0.        ]), 'sampling_rate': 16000}, 'speaker_id': 'G0004', 'prompt': 'So nag iwan sila ng ilang CDs tapos sabi ipatch nlng daw.', 'sentence': 'so nag-iwan sila ng ilang C Ds tapos sabi i-patch na lang daw', 'gender': 'M', 'age': 24, 'region_city': 'Cordillera Administrative Region, Baguio', 'device': 'AKG'}\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"Filipino_Scripted_Speech_Corpus_Daily_Use_Sentence\"])\n",
    "print(datasets[\"Filipino_Scripted_Speech_Corpus_Daily_Use_Sentence\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e242a28f639347668906168857b6da26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0610430cc54c0db63fa6bd72882852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc67d3390604bc98024b05ae85f8509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04579cf133747169f340459a7e48195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ddd4fc9e16479690f7ef90414fae00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afd9a6f208c4229a27276517e87b30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f573ee432a34980baa3b96ab43d94f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb945c541954ad897269206d40fef6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/keeve101/magic-hub-ms-tl-datasets/commit/8b43a10661b084506614620827b141b703d3c8c6', commit_message='Upload dataset', commit_description='', oid='8b43a10661b084506614620827b141b703d3c8c6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/keeve101/magic-hub-ms-tl-datasets', endpoint='https://huggingface.co', repo_type='dataset', repo_id='keeve101/magic-hub-ms-tl-datasets'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ms_dataset = concatenate_datasets([dataset for key, dataset in datasets.items() if \"Malay\" in key])\n",
    "combined_ms_dataset = combined_ms_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "combined_ms_dataset.push_to_hub(\"keeve101/magic-hub-ms-tl-datasets\", \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfb0eed86254a2889d1b84ea57b2acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff76449d6407474797fca31fc960b09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca948c721b3494d90cab7ebe0af6c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fed15323544dbdaacbd11c784be65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9476df73b033467e88f25c6028b53d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026d16e8189b41afbf0b10c9cb6e6b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5df1b7439f14b269ea20ba374ae2aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4290110a65445ba77539fd830ee8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea2859e2a8948b8b52a4c604697e80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/697 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keith\\anaconda3\\envs\\llmchat\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\keith\\.cache\\huggingface\\hub\\datasets--keeve101--magic-hub-ms-tl-datasets. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in datasets.items():\n",
    "    if \"Filipino\" in key:\n",
    "        dataset = dataset.train_test_split(test_size=0.1)\n",
    "        dataset.push_to_hub(\"keeve101/magic-hub-ms-tl-datasets\", \"tl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
